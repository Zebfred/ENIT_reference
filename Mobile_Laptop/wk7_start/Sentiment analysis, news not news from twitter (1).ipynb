{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional,Flatten,Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    return html_free\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation and c not in string.digits])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('uw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         screams in  different languages\n",
       "1       Families to sue over Legionnaires More than  f...\n",
       "2       Pandemonium In Aba As Woman Delivers Baby With...\n",
       "3       My emotions are a train wreck My body is a tra...\n",
       "4       Alton brown just did a livestream and he burne...\n",
       "                              ...                        \n",
       "1859    Trollkrattos Juan Carlos Salvador The Secret T...\n",
       "1860    devonbreneman hopefully it doesnt electrocute ...\n",
       "1861    Businesses are deluged with invokces Make your...\n",
       "1862    BREAKING  police officers arrested for abusing...\n",
       "1863    News Refugio oil spill may have been costlier ...\n",
       "Name: clean, Length: 1864, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['Input'].apply(lambda x: remove_punctuation(x))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [screams, in, different, languages]\n",
       "1       [families, to, sue, over, legionnaires, more, ...\n",
       "2       [pandemonium, in, aba, as, woman, delivers, ba...\n",
       "3       [my, emotions, are, a, train, wreck, my, body,...\n",
       "4       [alton, brown, just, did, a, livestream, and, ...\n",
       "                              ...                        \n",
       "1859    [trollkrattos, juan, carlos, salvador, the, se...\n",
       "1860    [devonbreneman, hopefully, it, doesnt, electro...\n",
       "1861    [businesses, are, deluged, with, invokces, mak...\n",
       "1862    [breaking, police, officers, arrested, for, ab...\n",
       "1863    [news, refugio, oil, spill, may, have, been, c...\n",
       "Name: clean, Length: 1864, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: tokenizer1.tokenize(x.lower()))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [screams, different, languages]\n",
       "1       [families, sue, legionnaires, families, affect...\n",
       "2       [pandemonium, aba, woman, delivers, baby, with...\n",
       "3       [emotions, train, wreck, body, train, wreck, i...\n",
       "4       [alton, brown, livestream, burned, butter, tou...\n",
       "                              ...                        \n",
       "1859    [trollkrattos, juan, carlos, salvador, secret,...\n",
       "1860    [devonbreneman, hopefully, doesnt, electrocute...\n",
       "1861    [businesses, deluged, invokces, make, stand, c...\n",
       "1862    [breaking, police, officers, arrested, abusing...\n",
       "1863    [news, refugio, oil, spill, may, costlier, big...\n",
       "Name: clean, Length: 1864, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_stopwords(x))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000,lower=True,split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['Input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Input'])\n",
    "X = pad_sequences(X,maxlen=500)\n",
    "Y = df['Validation']\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    le.fit(y_test)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    \n",
    "    return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_test = prepare_targets(Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 50)           431850    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 615,403\n",
      "Trainable params: 615,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=500))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.1442 - accuracy: 0.9586 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/6\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.0880 - accuracy: 0.9716 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/6\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/6\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/6\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/6\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "history=model.fit(X_train, y_train, batch_size=128, epochs=6, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94667554]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = 'ten killed and hundreds wounded in the earthquake'\n",
    "x_1=tokenizer.texts_to_sequences([news])\n",
    "x_1 = pad_sequences(x_1,maxlen=500)\n",
    "model.predict(x_1)\n",
    "#the output is closer to 1 for news\n",
    "#and closer to zero for not news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01637271]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news2 = 'going to the beach this weekend'\n",
    "x_2=tokenizer.texts_to_sequences([news2])\n",
    "x_2 = pad_sequences(x_2,maxlen=500)\n",
    "model.predict(x_2)\n",
    "#the output is closer to 1 for news\n",
    "#and closer to zero for not news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('news.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
