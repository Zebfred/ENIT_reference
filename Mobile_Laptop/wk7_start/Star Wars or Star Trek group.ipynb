{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional,Flatten,Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    return html_free\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation and c not in string.digits])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wars_trek.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Input'] = df['Input'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        turmoil has engulfed the galactic republic the...\n",
       "1                     outlaying star systems is in dispute\n",
       "2        hoping to resolve the matter with a blockade o...\n",
       "3        greedy trade federation has stopped all shippi...\n",
       "4                                                    naboo\n",
       "                               ...                        \n",
       "70419                         where no man has gone before\n",
       "70420     she is moving out now passing camera and heading\n",
       "70421    toward the distant stars she is beautiful and ...\n",
       "70422      are beautiful and as she slowly disappears from\n",
       "70423                                                 view\n",
       "Name: clean, Length: 70424, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['Input'].apply(lambda x: remove_punctuation(x))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [turmoil, has, engulfed, the, galactic, republ...\n",
       "1              [outlaying, star, systems, is, in, dispute]\n",
       "2        [hoping, to, resolve, the, matter, with, a, bl...\n",
       "3        [greedy, trade, federation, has, stopped, all,...\n",
       "4                                                  [naboo]\n",
       "                               ...                        \n",
       "70419                  [where, no, man, has, gone, before]\n",
       "70420    [she, is, moving, out, now, passing, camera, a...\n",
       "70421    [toward, the, distant, stars, she, is, beautif...\n",
       "70422    [are, beautiful, and, as, she, slowly, disappe...\n",
       "70423                                               [view]\n",
       "Name: clean, Length: 70424, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: tokenizer1.tokenize(x.lower()))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [turmoil, engulfed, galactic, republic, taxati...\n",
       "1                      [outlaying, star, systems, dispute]\n",
       "2        [hoping, resolve, matter, blockade, deadly, ba...\n",
       "3        [greedy, trade, federation, stopped, shipping,...\n",
       "4                                                  [naboo]\n",
       "                               ...                        \n",
       "70419                                          [man, gone]\n",
       "70420                   [moving, passing, camera, heading]\n",
       "70421                  [toward, distant, stars, beautiful]\n",
       "70422                      [beautiful, slowly, disappears]\n",
       "70423                                               [view]\n",
       "Name: clean, Length: 70424, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_stopwords(x))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000,lower=True,split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['Input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Input'])\n",
    "X = pad_sequences(X,maxlen=500)\n",
    "Y = df['Value']\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(xtrain,labels,test_size=0.2,train_size=0.8)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    le.fit(y_test)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    \n",
    "    return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_test = prepare_targets(Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 50)           909200    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,092,753\n",
      "Trainable params: 1,092,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=500)) \n",
    "model.add(Bidirectional(LSTM(128))) #number of batches\n",
    "model.add(Dropout(0.5)) #randomily removing some of the neurons from the architecture to decrease overfiting\n",
    "model.add(Dense(1,activation='sigmoid')) #it regulates the outputs \n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy']) #regulate dropout\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "386/386 [==============================] - 1170s 3s/step - loss: 0.1486 - accuracy: 0.9446 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "386/386 [==============================] - 1240s 3s/step - loss: 0.1391 - accuracy: 0.9483 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "386/386 [==============================] - 1286s 3s/step - loss: 0.1301 - accuracy: 0.9506 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "history=model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('stars12.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 96s 144ms/step - loss: 0.1764 - accuracy: 0.9366\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
