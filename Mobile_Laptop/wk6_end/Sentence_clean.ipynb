{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trek = pd.read_json('all_series_lines.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DS9</th>\n",
       "      <th>TOS</th>\n",
       "      <th>TAS</th>\n",
       "      <th>TNG</th>\n",
       "      <th>VOY</th>\n",
       "      <th>ENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>episode 0</th>\n",
       "      <td>{'PICARD': ['Come.', 'Commander. Yes, please, ...</td>\n",
       "      <td>{'SURVIVOR': ['Is Earth all right?'], 'PIKE': ...</td>\n",
       "      <td>{'ENGINEER': ['Yes, sir.'], 'KIRK': ['Situatio...</td>\n",
       "      <td>{'PICARD': ['You will agree, Data, that Starfl...</td>\n",
       "      <td>{'CREWWOMAN': ['Aye, Captain.'], 'NURSE': ['He...</td>\n",
       "      <td>{'HOSHI': ['Ghlungit ! tak nekleet.', 'Very go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 1</th>\n",
       "      <td>{'PICARD': [], 'ODO': ['What?', 'And we have s...</td>\n",
       "      <td>{'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...</td>\n",
       "      <td>{'ENGINEER': [], 'KIRK': ['What a trip, Bones....</td>\n",
       "      <td>{'PICARD': ['Report.', 'Are you certain? Yes, ...</td>\n",
       "      <td>{'CREWWOMAN': [], 'NURSE': [], 'EMH': ['Your n...</td>\n",
       "      <td>{'HOSHI': ['Wish I did. She doesn't look any b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 2</th>\n",
       "      <td>{'PICARD': [], 'ODO': ['Business is good, Quar...</td>\n",
       "      <td>{'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...</td>\n",
       "      <td>{'ENGINEER': [], 'KIRK': ['Readings?', 'Put up...</td>\n",
       "      <td>{'PICARD': ['On our way. You have the helm, Mi...</td>\n",
       "      <td>{'CREWWOMAN': [], 'NURSE': [], 'EMH': ['Hmm. H...</td>\n",
       "      <td>{'HOSHI': ['Suitable for humanoid life.', 'Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 3</th>\n",
       "      <td>{'PICARD': [], 'ODO': ['Tell me, Quark, am I m...</td>\n",
       "      <td>{'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...</td>\n",
       "      <td>{'ENGINEER': [], 'KIRK': ['Twenty seconds to w...</td>\n",
       "      <td>{'PICARD': ['Enlarge. What is their course?', ...</td>\n",
       "      <td>{'CREWWOMAN': [], 'NURSE': [], 'EMH': ['Equipm...</td>\n",
       "      <td>{'HOSHI': ['I wonder how long they've been the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 4</th>\n",
       "      <td>{'PICARD': [], 'ODO': ['Just what do you think...</td>\n",
       "      <td>{'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...</td>\n",
       "      <td>{'ENGINEER': [], 'KIRK': ['Klingon battle crui...</td>\n",
       "      <td>{'PICARD': ['I don't understand your concern, ...</td>\n",
       "      <td>{'CREWWOMAN': ['Sorry.'], 'NURSE': [], 'EMH': ...</td>\n",
       "      <td>{'HOSHI': ['If you don't look too close, you'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 171</th>\n",
       "      <td>{'PICARD': [], 'ODO': ['Finally. The last time...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'PICARD': ['As a result, we won't be able to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 172</th>\n",
       "      <td>{'PICARD': [], 'ODO': ['Mind if I walk with yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'PICARD': ['Report.', 'On screen.', 'Open a c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 173</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'PICARD': ['Data, I can barely see.', 'Yes, b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 174</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'PICARD': ['Lieutenant Ro. RO', 'Please repor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 175</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'PICARD': ['Counsellor! What's today's date? ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           DS9  \\\n",
       "episode 0    {'PICARD': ['Come.', 'Commander. Yes, please, ...   \n",
       "episode 1    {'PICARD': [], 'ODO': ['What?', 'And we have s...   \n",
       "episode 2    {'PICARD': [], 'ODO': ['Business is good, Quar...   \n",
       "episode 3    {'PICARD': [], 'ODO': ['Tell me, Quark, am I m...   \n",
       "episode 4    {'PICARD': [], 'ODO': ['Just what do you think...   \n",
       "...                                                        ...   \n",
       "episode 171  {'PICARD': [], 'ODO': ['Finally. The last time...   \n",
       "episode 172  {'PICARD': [], 'ODO': ['Mind if I walk with yo...   \n",
       "episode 173                                                NaN   \n",
       "episode 174                                                NaN   \n",
       "episode 175                                                NaN   \n",
       "\n",
       "                                                           TOS  \\\n",
       "episode 0    {'SURVIVOR': ['Is Earth all right?'], 'PIKE': ...   \n",
       "episode 1    {'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...   \n",
       "episode 2    {'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...   \n",
       "episode 3    {'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...   \n",
       "episode 4    {'SURVIVOR': [], 'PIKE': [], 'ORION': [], 'TYL...   \n",
       "...                                                        ...   \n",
       "episode 171                                                NaN   \n",
       "episode 172                                                NaN   \n",
       "episode 173                                                NaN   \n",
       "episode 174                                                NaN   \n",
       "episode 175                                                NaN   \n",
       "\n",
       "                                                           TAS  \\\n",
       "episode 0    {'ENGINEER': ['Yes, sir.'], 'KIRK': ['Situatio...   \n",
       "episode 1    {'ENGINEER': [], 'KIRK': ['What a trip, Bones....   \n",
       "episode 2    {'ENGINEER': [], 'KIRK': ['Readings?', 'Put up...   \n",
       "episode 3    {'ENGINEER': [], 'KIRK': ['Twenty seconds to w...   \n",
       "episode 4    {'ENGINEER': [], 'KIRK': ['Klingon battle crui...   \n",
       "...                                                        ...   \n",
       "episode 171                                                NaN   \n",
       "episode 172                                                NaN   \n",
       "episode 173                                                NaN   \n",
       "episode 174                                                NaN   \n",
       "episode 175                                                NaN   \n",
       "\n",
       "                                                           TNG  \\\n",
       "episode 0    {'PICARD': ['You will agree, Data, that Starfl...   \n",
       "episode 1    {'PICARD': ['Report.', 'Are you certain? Yes, ...   \n",
       "episode 2    {'PICARD': ['On our way. You have the helm, Mi...   \n",
       "episode 3    {'PICARD': ['Enlarge. What is their course?', ...   \n",
       "episode 4    {'PICARD': ['I don't understand your concern, ...   \n",
       "...                                                        ...   \n",
       "episode 171  {'PICARD': ['As a result, we won't be able to ...   \n",
       "episode 172  {'PICARD': ['Report.', 'On screen.', 'Open a c...   \n",
       "episode 173  {'PICARD': ['Data, I can barely see.', 'Yes, b...   \n",
       "episode 174  {'PICARD': ['Lieutenant Ro. RO', 'Please repor...   \n",
       "episode 175  {'PICARD': ['Counsellor! What's today's date? ...   \n",
       "\n",
       "                                                           VOY  \\\n",
       "episode 0    {'CREWWOMAN': ['Aye, Captain.'], 'NURSE': ['He...   \n",
       "episode 1    {'CREWWOMAN': [], 'NURSE': [], 'EMH': ['Your n...   \n",
       "episode 2    {'CREWWOMAN': [], 'NURSE': [], 'EMH': ['Hmm. H...   \n",
       "episode 3    {'CREWWOMAN': [], 'NURSE': [], 'EMH': ['Equipm...   \n",
       "episode 4    {'CREWWOMAN': ['Sorry.'], 'NURSE': [], 'EMH': ...   \n",
       "...                                                        ...   \n",
       "episode 171                                                NaN   \n",
       "episode 172                                                NaN   \n",
       "episode 173                                                NaN   \n",
       "episode 174                                                NaN   \n",
       "episode 175                                                NaN   \n",
       "\n",
       "                                                           ENT  \n",
       "episode 0    {'HOSHI': ['Ghlungit ! tak nekleet.', 'Very go...  \n",
       "episode 1    {'HOSHI': ['Wish I did. She doesn't look any b...  \n",
       "episode 2    {'HOSHI': ['Suitable for humanoid life.', 'Not...  \n",
       "episode 3    {'HOSHI': ['I wonder how long they've been the...  \n",
       "episode 4    {'HOSHI': ['If you don't look too close, you'd...  \n",
       "...                                                        ...  \n",
       "episode 171                                                NaN  \n",
       "episode 172                                                NaN  \n",
       "episode 173                                                NaN  \n",
       "episode 174                                                NaN  \n",
       "episode 175                                                NaN  \n",
       "\n",
       "[176 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'english3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-79e4b3f89dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfileref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english3.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfileref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'english3.txt'"
     ]
    }
   ],
   "source": [
    "fileref = open('english3.txt', 'r', encoding=\"utf8\")\n",
    "all_words = fileref.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = word_tokenize(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    return html_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation and c not in string.digits])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_not_english(text):\n",
    "    eng_only = [w for w in text if w in all_words]\n",
    "    return eng_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_textbdf['clean'] = df['Input'].apply(lambda x: remove_punctuation(x))\n",
    "df['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['Input'].apply(lambda x: remove_punctuation(x))\n",
    "df['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_html(x))\n",
    "#df['Input'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_stopwords(x))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['clean'].apply(lambda x: remove_not_english(x))\n",
    "df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed'] = df['clean'].apply(lambda x: word_stemmer(x))\n",
    "df['stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmazed'] = df['clean'].apply(lambda x: word_lemmatizer(x))\n",
    "df['lemmazed']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
