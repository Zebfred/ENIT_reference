{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want this project done\n",
    "\n",
    "I don't want to be a fake anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B = np.array([[67, 158], [67, 170], [67, 175]])\n",
    "gnb2 = GaussNB()\n",
    "gnb2.fit(X,y)\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c = y_hat, alpha = 0.15)\n",
    "plt.scatter(B[:,0], B[:,1], alpha = 0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_hat2 = gnb2.predict(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c = y_hat, alpha = 0.15)\n",
    "plt.scatter(B[:,0], B[:,1], c = y_hat2, alpha = 0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import multinomial as mlvn\n",
    "from scipy.stats import bernoulli as brn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Zebfred\\\\Enhanced_IT'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"donnut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"xor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.823942360433377097e+00 -1.146120866951919892e-01 0.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.502854598957227061e-01 1.064485258017476843e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.577960703558637601e+00 1.125407965407776434e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.932518160242990390e-01 -5.476939453545561776...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.463078900648939751e+00 -8.489925711833897770...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.170321448869729775e+00 9.380680932182686727e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  1.823942360433377097e+00 -1.146120866951919892e-01 0.000000000000000000e+00\n",
       "0  7.502854598957227061e-01 1.064485258017476843e...                         \n",
       "1  1.577960703558637601e+00 1.125407965407776434e...                         \n",
       "2  3.932518160242990390e-01 -5.476939453545561776...                         \n",
       "3  2.463078900648939751e+00 -8.489925711833897770...                         \n",
       "4  1.170321448869729775e+00 9.380680932182686727e...                         "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-80a4aab09c73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X[:,-1]\n",
    "X = X[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(3999, 0), dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.823942360433377097e+00 -1.146120866951919892e-01 0.000000000000000000e+00']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = data.columns.values.tolist()\n",
    "some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2a6cfe714855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    }
   ],
   "source": [
    "X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1795ad2be50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X, X, alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussNB():\n",
    "    def fit(self, X, y, epsilon = 1e-3):\n",
    "        self.likelihoods = dict()\n",
    "        self.priors = dict()\n",
    "        \n",
    "        self.K = set(y.astype(int))\n",
    "        \n",
    "        for k in self.K:\n",
    "            X_k = X[y == k,:]\n",
    "            self.likelihoods[k] = {\"mean\":X_k.mean(axis=0), \"cov\":X_k.var(axis=0) + epsilon}\n",
    "            self.priors[k] = len(X_k)/len(X)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        \n",
    "        P_hat = np.zeros((N,len(self.K)))\n",
    "        \n",
    "        for k, l in self.likelihoods.items():\n",
    "            P_hat[:,k] = mvn.logpdf(X, l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k])\n",
    "        return P_hat.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussNB()\n",
    "gnb.fit(X,y)\n",
    "y_hat = gnb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c = y_hat, alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy(y, y_hat):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussBayes():\n",
    "    def fit(self, X, y, epsilon = 1e-3):\n",
    "        self.likelihoods = dict()\n",
    "        self.priors = dict()\n",
    "        \n",
    "        self.K = set(y.astype(int))\n",
    "        \n",
    "        for k in self.K:\n",
    "            X_k = X[y == k,:]\n",
    "            N_k, D = X_k.shape\n",
    "            mu_k=X_k.mean(axis=0)\n",
    "            self.likelihoods[k] = {\"mean\":X_k.mean(axis=0), \"cov\":(1/(N_k-1))*np.matmul((X_k-mu_k).T,X_k-mu_k)+ epsilon*np.identity(D)}\n",
    "            self.priors[k] = len(X_k)/len(X)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        \n",
    "        P_hat = np.zeros((N,len(self.K)))\n",
    "        \n",
    "        for k, l in self.likelihoods.items():\n",
    "            P_hat[:,k] = mvn.logpdf(X, l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k])\n",
    "            \n",
    "        return P_hat.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussBayes()\n",
    "gnb.fit(X,y)\n",
    "y_hat = gnb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c = y_hat, alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy(y, y_hat):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenGaussBayes():\n",
    "    def fit(self, X, y, epsilon = 1e-3):\n",
    "        self.likelihoods = dict()\n",
    "        self.priors = dict()\n",
    "        \n",
    "        self.K = set(y.astype(int))\n",
    "        \n",
    "        for k in self.K:\n",
    "            X_k = X[y == k,:]\n",
    "            N_k, D = X_k.shape\n",
    "            mu_k=X_k.mean(axis=0)\n",
    "            self.likelihoods[k] = {\"mean\":X_k.mean(axis=0), \"cov\":(1/(N_k-1))*np.matmul((X_k-mu_k).T,X_k-mu_k)+ epsilon*np.identity(D)}\n",
    "            self.priors[k] = len(X_k)/len(X)\n",
    "            \n",
    "    def predict(self, X,DistFam):\n",
    "        N, D = X.shape\n",
    "        \n",
    "        P_hat = np.zeros((N,len(self.K)))\n",
    "        \n",
    "        for k, l in self.likelihoods.items():\n",
    "            P_hat[:,k] = DistFam(X, l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k])\n",
    "            \n",
    "        return P_hat.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DSel(dstring):\n",
    "    if dstring==\"Gauss\" or dstring== \"Gaussian\" or dstring==\"gauss\" or dstring==\"gaussian\":return mvn.logpdf\n",
    "    if dstring==\"multi\" or dstring== \"Multinomial\" or dstring==\"multinomial\" or dstring==\"Multi\":return mlvn.logpmf   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c = y_hat, alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenMultBayes():\n",
    "    \n",
    "    def fit(self, X, y,DistStr, epsilon = 1e-3):\n",
    "        self.likelihoods = dict()\n",
    "        self.priors = dict()\n",
    "        \n",
    "        self.K = set(y.astype(int))\n",
    "        if DistStr==\"Gauss\":\n",
    "        \n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:]\n",
    "                N_k, D = X_k.shape\n",
    "                mu_k=X_k.mean(axis=0)\n",
    "                self.likelihoods[k] = {\"mean\":X_k.mean(axis=0), \"cov\":(1/(N_k-1))*np.matmul((X_k-mu_k).T,X_k-mu_k)+ epsilon*np.identity(D)}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "            return\n",
    "        if DisStr==\"Multinomial\":\n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:]\n",
    "                N_k, D = X_k.shape\n",
    "                mu_k=X_k.mean(axis=0)\n",
    "                self.likelihoods[k] = {\"N\":N, \"P\":sum(N_k/len(X))}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "            \n",
    "        if DistStr==\"Bernoulli\":\n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:]\n",
    "                N_k, D = X_k.shape\n",
    "               \n",
    "                self.likelihoods[k] = {\"P\":N_k/len(X)}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "            \n",
    "    def predict(self, X,DistStr):\n",
    "        N, D = X.shape\n",
    "        \n",
    "        if DistStr==\"Gauss\":\n",
    "            P_hat = np.zeros((N,len(self.K)))\n",
    "\n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = mvn.logpdf(X, l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k])\n",
    "\n",
    "            return P_hat.argmax(axis = 1)\n",
    "        \n",
    "        if DisStr==\"Multinomial\":\n",
    "            P_hat = np.zeros((N,len(self.K)))\n",
    "\n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = mlvn.logpmf(X, l[\"N\"], l[\"P\"]) + np.log(self.priors[k])\n",
    "\n",
    "            return P_hat.argmax(axis = 1)\n",
    "\n",
    "        if DistStr==\"Bernoulli\":\n",
    "            P_hat = np.zeros((N,len(self.K)))\n",
    "\n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = bernoulli.logpmf(X,l[\"P\"]) + np.log(self.priors[k])\n",
    "\n",
    "            return P_hat.argmax(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnb = GenMultBayes()\n",
    "gnb.fit(X,y)\n",
    "y_hat = gnb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "\n",
    "\tdef __init__(self, k =3, tolerance = 0.0001, max_iterations = 500):\n",
    "\t\tself.k = k\n",
    "\t\tself.tolerance = tolerance\n",
    "\t\tself.max_iterations = max_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I will be using the Euclidean distance as the distance metric (through there are other options such as the Manhattan Distance, Minkowski Distance ). Th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "From Wikipedia:\n",
    "\n",
    "In Cartesian coordinates, if p = (p1, p2,…, pn) and q = (q1, q2,…, qn) are two points in Euclidean n-space, then the distance (d) from p to q , or from q to p is given by:\n",
    "\n",
    "\n",
    "\n",
    "Implementing Euclidean Distance For Two Features In Python:\n",
    "import math\n",
    "\n",
    "def Euclidean_distance(feat_one, feat_two):\n",
    "\n",
    "    squared_distance = 0\n",
    "\n",
    "    #Assuming correct input to the function where the lengths of two features are the same\n",
    "\n",
    "    for i in range(len(feat_one)):\n",
    "\n",
    "            squared_distance += (feat_one[i] – feat_two[i])**2\n",
    "\n",
    "    ed = sqrt(squared_distances)\n",
    "\n",
    "    return ed;\n",
    "The above code can be extended to n number of features. In this example, however, I will rely on Python’s numpy library’s function: numpy.linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def Euclidean_distance(feat_one, feat_two):\n",
    "\n",
    "    squared_distance = 0\n",
    "\n",
    "    #Assuming correct input to the function where the lengths of two features are the same\n",
    "\n",
    "    for i in range(len(feat_one)):\n",
    "\n",
    "            squared_distance += (feat_one[i] – feat_two[i])**2\n",
    "\n",
    "    ed = sqrt(squared_distances)\n",
    "\n",
    "    return ed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering:\n",
    "After figuring out the distances between the points, we will use the distances to find which cluster amongst the k clusters a given data point belongs to.\n",
    "\n",
    "First, let’s initialize the centroids randomly:\n",
    "\n",
    "#initialize the centroids, the first 'k' elements in the dataset will be our initial centroids\n",
    "for i in range(self.k):\n",
    "\tself.centroids[i] = data[i]\n",
    "Now, let’s enter the main loop.\n",
    "\n",
    "for i in range(self.max_iterations):\n",
    "\t\tself.classes = {}\n",
    "\t\tfor i in range(self.k):\n",
    "\t\t\tself.classes[i] = []\n",
    "\n",
    "\t\t#find the distance between the point and cluster; choose the nearest centroid\n",
    "\t\tfor features in data:\n",
    "\t\t\tdistances = [np.linalg.norm(features - self.centroids[centroid]) for centroid in self.centroids]\n",
    "\t\t\tclassification = distances.index(min(distances))\n",
    "\t\t\tself.classes[classification].append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(self.k):\n",
    "\tself.centroids[i] = data[i]\n",
    "Now, let’s enter the main loop.\n",
    "\n",
    "for i in range(self.max_iterations):\n",
    "\t\tself.classes = {}\n",
    "\t\tfor i in range(self.k):\n",
    "\t\t\tself.classes[i] = []\n",
    "\n",
    "\t\t#find the distance between the point and cluster; choose the nearest centroid\n",
    "\t\tfor features in data:\n",
    "\t\t\tdistances = [np.linalg.norm(features - self.centroids[centroid]) for centroid in self.centroids]\n",
    "\t\t\tclassification = distances.index(min(distances))\n",
    "\t\t\tself.classes[classification].append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let’s re-calculate the cluster centroids.\n",
    "\n",
    "previous = dict(self.centroids)\n",
    "\n",
    "#average the cluster datapoints to re-calculate the centroids\n",
    "for classification in self.classes:\n",
    "\tself.centroids[classification] = np.average(self.classes[classification], axis = 0)\n",
    "#The dictionary previous stores the value of centroids that the previous iteration returned, we performed the clustering in this iteration based on these centroids. Then we iterate though the classes list and find the average of all the datapoints in the given cluster. This is, perhaps, the machine learning part of k-means. The algorithm recomputes the centroids as long as it’s optimal(or if there have been far too many interations in attempting to do so).\n",
    "\n",
    "Time to see if our algorithm has reached the optimal values of centroids. For this, let’s have a flag isOptimal.\n",
    "\n",
    "isOptimal = True\n",
    "#Let’s iterate though the new centroids and compare it with the older centroid values and see if it’s converged.\n",
    "\n",
    "for centroid in self.centroids:\n",
    "\n",
    "\toriginal_centroid = previous[centroid]\n",
    "\tcurr = self.centroids[centroid]\n",
    "\n",
    "\tif np.sum((curr - original_centroid)/original_centroid * 100.0) > self.tolerance:\n",
    "\t\tisOptimal = False\n",
    "\n",
    "\t#break out of the main loop if the results are optimal, ie. the centroids don't change their positions much(more than our tolerance)\n",
    "\n",
    "if isOptimal:\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In main:\n",
    "\n",
    "km = K_Means(3)\n",
    "km.fit(X)\n",
    "\n",
    "# Plotting starts here, the colors\n",
    "colors = 10*[\"r\", \"g\", \"c\", \"b\", \"k\"]\n",
    "Lets mark our centroids with an x.\n",
    "\n",
    "for centroid in km.centroids:\n",
    "\tplt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], s = 130, marker = \"x\")\n",
    "Now, let’s go ahead and plot the datapoints and color them based on their cluster.\n",
    "\n",
    "for classification in km.classes:\n",
    "\tcolor = colors[classification]\n",
    "\tfor features in km.classes[classification]:\n",
    "\t\tplt.scatter(features[0], features[1], color = color,s = 30)\n",
    "Show the plot:\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \n",
    "    def __init__(self ,k = 8, max_iter = 100 ):\n",
    "        self.k = k\n",
    "        self.max_iter = max_iter\n",
    "        print(\"Initalized k with :\",k)\n",
    "        \n",
    "    def euclidDistance(self , x1, x2):\n",
    "    return (np.sqrt(np.sum(np.square(x1 - x2) , axis = 1)))\n",
    "\n",
    "    def fit(self, data):\n",
    "    self.centroids = []\n",
    "        \n",
    "    #initialize the centroids, the first 'k' elements in the dataset will be our initial centroids\n",
    "\n",
    "    for i in range(self.k):\n",
    "        self.centroids.append(data.iloc[i].to_numpy())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.classes = {}\n",
    "for cluster in range(self.k):\n",
    "      self.classes[cluster] = []\n",
    "\n",
    "      #find the distance between the point and cluster; choose the nearest centroid\n",
    "      for point in range(len(data)):\n",
    "           distances = self.euclidDistance(self.centroids, data.iloc[point].to_numpy())\n",
    "           classification = np.argmin(distances)\n",
    "           self.classes[classification].append(data.iloc[point])\n",
    "        \n",
    "previous = np.array(self.centroids)\n",
    "#average the cluster datapoints to re-calculate the centroids\n",
    "for classification in self.classes:\n",
    "       self.centroids[classification] = np.average(self.classes[classification], axis = 0)\n",
    "            \n",
    "optimal = True\n",
    "curr = np.array(self.centroids)\n",
    "            \n",
    "#difference in the cluster centers of two consecutive iterations to declare convergence.\n",
    "if np.sum((curr - previous)/previous * 100.0) > 0.0001:\n",
    "      optimal = False\n",
    "\n",
    "          \n",
    "#break out of the main loop if the results are optimal, ie. the centroids don't change their positions much(more than our tolerance)\n",
    "if optimal:\n",
    "      break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fication = np.argmin(distances)\n",
    "            self.classes[classification].append(data.iloc[point])\n",
    "\n",
    "        previous = np.array(self.centroids)\n",
    "        #average the cluster datapoints to re-calculate the centroids\n",
    "        for classification in self.classes:\n",
    "            self.centroids[classification] = np.average(self.classes[classification], axis = 0)\n",
    "        \n",
    "        optimal = True\n",
    "        curr = np.array(self.centroids)\n",
    "        \n",
    "        #difference in the cluster centers of two consecutive iterations to declare convergence.\n",
    "        if np.sum((curr - previous)/previous * 100.0) > 0.0001:\n",
    "            optimal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.offline as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "\n",
    "def k_means_clustering(path, k):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[['V1', 'V2']]\n",
    "    k_means = (data.sample(k, replace=False))\n",
    "    k_means2 = pd.DataFrame()\n",
    "    clusters = pd.DataFrame()\n",
    "    print('Initial means:\\n', k_means)\n",
    "\n",
    "    while not k_means2.equals(k_means):\n",
    "\n",
    "        # distance matrix\n",
    "        cluster_count = 0\n",
    "        for idx, k_mean in k_means.iterrows():\n",
    "            clusters[cluster_count] = (data[k_means.columns] - np.array(k_mean)).pow(2).sum(1).pow(0.5)\n",
    "            cluster_count += 1\n",
    "\n",
    "        # update cluster\n",
    "        data['MDCluster'] = clusters.idxmin(axis=1)\n",
    "\n",
    "        # store previous cluster\n",
    "        k_means2 = k_means\n",
    "        k_means = pd.DataFrame()\n",
    "        k_means_frame = data.groupby('MDCluster').agg(np.mean)\n",
    "\n",
    "        k_means[k_means_frame.columns] = k_means_frame[k_means_frame.columns]\n",
    "\n",
    "        print(k_means.equals(k_means2))\n",
    "\n",
    "    # plotting\n",
    "    print('Plotting...')\n",
    "    data_graph = [go.Scatter(\n",
    "        x=data['V1'],\n",
    "        y=data['V2'].where(data['MDCluster'] == c),\n",
    "        mode='markers',\n",
    "        name='Cluster: ' + str(c)\n",
    "    ) for c in range(k)]\n",
    "\n",
    "    data_graph.append(\n",
    "        go.Scatter(\n",
    "            x=k_means['V1'],\n",
    "            y=k_means['V2'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color='#000000',\n",
    "            ),\n",
    "            name='Centroids of Clusters'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.plot(data_graph, filename='../output_files/cluster.html')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k_means_clustering(path='../datasets/k_means_clustering_test_1.csv', k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './iris.csv'\n",
    "data = pd.read_csv(file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(X.iloc[:,0] , X.iloc[:,1] , hue = data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "clf.fit(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
