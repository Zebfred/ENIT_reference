{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superior K-NN Classifier\n",
    "\n",
    "See the **Cosine-Similarity Model Analysis** notebook in this repo for the context of the development of this notebook.\n",
    "\n",
    "In this notebook, we implement a k-nearest-neighbors classifier that classifies images using a tree-based approach.  In using this approach, we notice the significant performance speedups this algorithm gives us over the standard \"brute-force\" algorithm, as well as performance speed-ups over the optimized Scikit-Learn K-NN classifier.\n",
    "\n",
    "In this algorithm we implement below, everything will be the same as the previous algorithm (including the use of cosine similarity), except for the use of a tree based system in the classification step.  Recall that under the cosine similarity distance metric a value closer to one means the two vectors are \"more similar\" than values closer to zero.\n",
    "\n",
    "We start by importing our libraries, and developing the updated algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import collections\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(\"mnist_train.csv\", \n",
    "                        delimiter=\",\")\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", \n",
    "                       delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the algorithm.\n",
    "We start by developing a function that will return our distance tree we can use in point classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with producing a method, produce_tree, that will take the input data and develop a \"distance tree\" that we can use in classifiction\n",
    "def produce_tree(data):\n",
    "    \"\"\"Take a data set, data, and return a distance tree array to be used for classification purposes\"\"\"\n",
    "    base_len = data.shape[1]\n",
    "    base = np.array([np.ones(base_len)])\n",
    "    \n",
    "    # distance list to sort and operate on\n",
    "    dist_ref = []\n",
    "    # dictionary of (distance-to-base, index value) pairs\n",
    "    distances = {}\n",
    "\n",
    "    for i in range(0, data.shape[0]):\n",
    "\n",
    "        curr = data[i]\n",
    "        dist = cosine_similarity([curr], base)\n",
    "        \n",
    "        # assign each to a dictionary\n",
    "        distances[dist[0][0]] = i\n",
    "        dist_ref.append(dist[0][0])\n",
    "\n",
    "    odistances = collections.OrderedDict(sorted(distances.items()))\n",
    "    dist_ref.sort()\n",
    "    \n",
    "    return dist_ref, odistances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Binary search method for the classification step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(a, x, lo=0, hi=None):  # can't use a to specify default for hi\n",
    "    hi = hi if hi is not None else len(a)  # hi defaults to len(a)   \n",
    "    pos = bisect_left(a, x, lo, hi)  # find insertion position\n",
    "    return (pos if pos != hi and a[pos] == x else pos)  # don't walk off the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our distance tree function, we may write the core K-NN algorithm that classifies each point efficiently using a binary search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_knn(k, dist_tree, dist_dict, test_data, stored_target):\n",
    "    \"\"\"k: number of neighbors to use for voting\n",
    "    dist_tree: the array of distance values used for classification\n",
    "    test_data: an unobserved image to classify\n",
    "    test_target: the label for the test_data (for calculating accuracy)\n",
    "    stored_data: the images already observed and available to the model\n",
    "    stored_target: labels for stored_data\n",
    "    \"\"\"\n",
    "    \n",
    "    # start by taking distance of input data and base vector to get distance\n",
    "    base = np.ones(len(test_data)).reshape(1, -1)\n",
    "    test_data = test_data.reshape(1, -1)\n",
    "    \n",
    "    # this is the distance value we will use in classifying it\n",
    "    similarity = cosine_similarity(base, test_data)\n",
    "    \n",
    "    # use binary search to find the index in dist_tree that it belongs to\n",
    "    indx = binary_search(dist_tree, similarity)\n",
    "    \n",
    "    # now get the set of k most similar data points\n",
    "    \n",
    "    # when going out around indx, make sure have room to expand out without going outside of list index range\n",
    "    top_room = len(dist_tree) - indx\n",
    "    bot_room = indx\n",
    "    k_rounded = 0\n",
    "    lower_bound = 0\n",
    "    upper_bound = 0\n",
    "    \n",
    "    if((k%2) == 0):\n",
    "        k_rounded = k/2\n",
    "    else:\n",
    "        k_rounded = round(k/2) + 1\n",
    "        \n",
    "    if((bot_room >= k_rounded) and (top_room >= k_rounded)):\n",
    "        lower_bound = int(indx-(k_rounded))\n",
    "        upper_bound = int(indx+(k_rounded))\n",
    "    \n",
    "    if(bot_room < k_rounded):\n",
    "        diff = k_rounded - bot_room\n",
    "        lower_bound = int(indx-(k_rounded - diff))\n",
    "        upper_bound = int(indx+(k_rounded + diff))\n",
    "        \n",
    "    if(top_room < k_rounded):\n",
    "        diff = k_rounded - top_room\n",
    "        lower_bound = int(indx-(k_rounded + diff))\n",
    "        upper_bound = int(indx+(k_rounded - diff))\n",
    "        \n",
    "    # now that we know we can safely get the right sub-sequence, get the subsequence of most similar points\n",
    "    most_sim = dist_tree[lower_bound:upper_bound+1]\n",
    "    \n",
    "    # now find the indices in our data set that correspond with the most similar points,\n",
    "    # and use those corresponding data point labels to determine the classification of the \n",
    "    # new point through voting\n",
    "    \n",
    "    # get the indices of the most similar\n",
    "    indices = []\n",
    "    for i in range(0, len(most_sim)):\n",
    "        curr = most_sim[i]\n",
    "        val = dist_dict[curr]\n",
    "        indices.append(val)\n",
    "        \n",
    "    # now get the corresponding labels of each of the most similar points given their indices\n",
    "    labels = [stored_target[i] for i in indices]   \n",
    "    labels = [i[0] for i in labels]\n",
    "    \n",
    "    # vote, and return prediction for every image in test_data\n",
    "    pred = np.bincount(labels)\n",
    "    pred = np.argmax(pred)\n",
    "    \n",
    "    # print table giving classifier accuracy using test_target\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function we can use to test the accuracy of our thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def classify(indx):\n",
    "    \"\"\"classifies the object in test_data[indx] by using the tree_knn method\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    dist_tree, dist_dict = produce_tree(train_data)\n",
    "    end = time.time()\n",
    "    print(\"Tree Construction Time \", end-start)\n",
    "    \n",
    "    start = time.time()\n",
    "    pred = tree_knn(8, dist_tree, dist_dict, test_data[indx], train_labels)\n",
    "    end = time.time()\n",
    "    print(\"\\nPoint Classification Time \", end-start)\n",
    "    \n",
    "    actual = test_labels[indx]\n",
    "    result = pred == actual\n",
    "    \n",
    "    print(\"\\n Actual: \", actual, \" Pred: \", pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Construction Time  8.014662981033325\n",
      "\n",
      "Point Classification Time  0.0006976127624511719\n",
      "\n",
      " Actual:  [4.]  Pred:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 10000\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "dist_tree, dist_dict = produce_tree(train_data)\n",
    "\n",
    "for i in range(0, test_data.shape[0]):\n",
    "    actual = test_labels[i]\n",
    "    pred = tree_knn(4, dist_tree, dist_dict, test_data[i], train_labels)\n",
    "    if(pred == actual):\n",
    "        correct += 1\n",
    "        \n",
    "print(correct, test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon analyzing the algorithm, it is clear that this algorithm severely lacks in classification accuracy.  This is because the comparison through a base vector does not capture enough information to make it work very well.  With that said, it does achieve the goal of classifying points much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
